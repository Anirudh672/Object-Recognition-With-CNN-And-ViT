{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDj9xoLEF5cH"
      },
      "source": [
        "#### DEEP LEARNING ASSIGNMENT 3<BR>\n",
        "\n",
        "SUBMITTED BY: ANIRUDH JOSHI (CS23MTECH11002)<br>\n",
        "DATE: 29/03/2024<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XejTwvtEF5cK"
      },
      "source": [
        "####  PROBLEM 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_aEF9YVF5cK"
      },
      "source": [
        "Self-Attention for Object Recognition with CNNs: Implement a sample CNN with one or\n",
        "more self-attention layer(s) for performing object recognition over CIFAR-10 dataset. You have to\n",
        "implement the self-attention layer yourself and use it in the forward function defined by you. All\n",
        "other layers (fully connected, nonlinearity, conv layer, etc.) can be bulit-in implementations. The\n",
        "network can be a simpler one (e.g., it may have 1x Conv, 4x [Conv followed by SA], 1x Conv, and\n",
        "1x GAP). Please refer to the reading material provided here or any other similar one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txau08O0F5cL"
      },
      "source": [
        "PREPARING DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyBMZT9ZF5cM",
        "outputId": "eba28888-d11b-41f9-a003-55bc728f5f94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 98781032.55it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# applying transformation to data after downloading it\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# downloading CIFAR-10 dataset\n",
        "full_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "full_testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ2enJO-F5cO"
      },
      "outputs": [],
      "source": [
        "# for testing purpose made small subset of training and testing data\n",
        "start_index = 0\n",
        "end_index = 5000\n",
        "\n",
        "# getting the smaller subset from the full train set\n",
        "subset_trainset = Subset(full_trainset, range(start_index, end_index))\n",
        "subset_testset = Subset(full_testset, range(start_index, end_index))\n",
        "\n",
        "# preparing the dataloaders for feeding into the network\n",
        "subset_trainloader = torch.utils.data.DataLoader(full_trainset, batch_size=100, shuffle=True)\n",
        "subset_testloader = torch.utils.data.DataLoader(full_testset, batch_size=100, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-A-mKIpF5cO"
      },
      "source": [
        "SELF-ATTENTION CLASS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mds1zfSjF5cO"
      },
      "source": [
        "intialization of class takes two parameters, in_channels which is number of channels in the input data. k here is the reduction factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3GVUKh5F5cP"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    # intializing the parameters\n",
        "    def __init__(self, in_channels, k=8):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.k = k\n",
        "        self.in_channels = in_channels\n",
        "        # getting the query, key and value representations respectively for the input image.\n",
        "        self.theta = nn.Conv2d(in_channels, in_channels // k, kernel_size=1)    # kernel size is 1 as we want to treat every pixel as separate feature\n",
        "        self.phi = nn.Conv2d(in_channels, in_channels // k, kernel_size=1)      # kernel size 1 also reduces dimensoinality in termo of channels\n",
        "        self.g = nn.Conv2d(in_channels, in_channels // k, kernel_size=1)\n",
        "        self.o = nn.Conv2d(in_channels // k, in_channels, kernel_size=1)\n",
        "\n",
        "    # forward pass in self attention\n",
        "    def forward(self, x):\n",
        "        # getting input dimensions\n",
        "        batch_size, C, width, height = x.size()\n",
        "        theta = self.theta(x).view(batch_size, self.in_channels // self.k, -1)      # reshaped into 3D tensor for subsequent matrix multiplication\n",
        "\n",
        "        # applied max pool and then reshaped it into 3D tensor\n",
        "        phi = nn.functional.max_pool2d(self.phi(x), 2).view(batch_size, self.in_channels // self.k, -1)\n",
        "        g = nn.functional.max_pool2d(self.g(x), 2).view(batch_size, self.in_channels // self.k, -1)\n",
        "\n",
        "        # bmm is batch matrix multiplixation between queries and keys tensor to compute attention scores\n",
        "        beta = nn.functional.softmax(torch.bmm(theta.permute(0, 2, 1), phi), dim=-1)\n",
        "\n",
        "        # applying the attention scores obtained to the values, generating final attended output\n",
        "        o = self.o(torch.bmm(g, beta.permute(0, 2, 1)).view(batch_size, self.in_channels // self.k, width, height))\n",
        "\n",
        "        return x + o\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ_yJUeSF5cP"
      },
      "source": [
        "DEFINING CNN WITH SPECIFIED LAYERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiAjfKLpF5cP"
      },
      "source": [
        "Class CNN with self attention uses the above defined self attention class and the inbuilt convolution and pooling layers. Eacg convolution layer is followed by a self attention layer. Initially output channels are increased in order to capture more complex features, also helpes in regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBSydnpfF5cQ"
      },
      "outputs": [],
      "source": [
        "# Class CNN with self attention\n",
        "class CNNWithSelfAttention(nn.Module):\n",
        "    # defining the convolution layers, pooling layers, fully connected and self attention layers.\n",
        "    def __init__(self):\n",
        "        super(CNNWithSelfAttention, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)     # conolution layer 1\n",
        "        self.sa1 = SelfAttention(32)                                # self attention 1\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.sa2 = SelfAttention(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.sa3 = SelfAttention(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.sa4 = SelfAttention(256)\n",
        "        self.conv5 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
        "        self.sa5 = SelfAttention(128)                               # final self attenion layer\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)              # global average pooling layer\n",
        "        self.fc = nn.Linear(128, 10)                                # fully connected layer\n",
        "\n",
        "    # forward pass of CNN\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        x = self.sa1(x)\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = self.sa2(x)\n",
        "        x = nn.functional.relu(self.conv3(x))\n",
        "        x = self.sa3(x)\n",
        "        x = nn.functional.relu(self.conv4(x))\n",
        "        x = self.sa4(x)\n",
        "        x = nn.functional.relu(self.conv5(x))\n",
        "        x = self.sa5(x)\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xazS0j3kF5cQ"
      },
      "source": [
        "TRAINING MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AHRTpzZF5cR"
      },
      "source": [
        "Initializing the model, loss function and the optimizer. As it is multi-class classification problem we use cross-entrpy loss function, as for the optimizer we use Adam with learning rate 0.001. Finally we feed the training data into model using already prepared training dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XiebLNYF5cR",
        "outputId": "d89df17f-95b2-41b5-cb30-e02ad3215622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   200] loss: 2.016\n",
            "[1,   400] loss: 1.732\n",
            "[2,   200] loss: 1.539\n",
            "[2,   400] loss: 1.476\n",
            "[3,   200] loss: 1.361\n",
            "[3,   400] loss: 1.308\n",
            "[4,   200] loss: 1.205\n",
            "[4,   400] loss: 1.183\n",
            "[5,   200] loss: 1.127\n",
            "[5,   400] loss: 1.087\n",
            "[6,   200] loss: 1.039\n",
            "[6,   400] loss: 1.011\n",
            "[7,   200] loss: 0.952\n",
            "[7,   400] loss: 0.945\n",
            "[8,   200] loss: 0.901\n",
            "[8,   400] loss: 0.886\n",
            "[9,   200] loss: 0.845\n",
            "[9,   400] loss: 0.850\n",
            "[10,   200] loss: 0.785\n",
            "[10,   400] loss: 0.813\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# Initializing the model, loss function and the optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNNWithSelfAttention().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# training model we run for 10 epoxhs\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(subset_trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        # printing every 200 batches\n",
        "        if i % 200 == 199:\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J785Tso0F5cR"
      },
      "source": [
        "TESTING MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKGbziZ6F5cS",
        "outputId": "aca2a99b-4946-4e61-d103-063674484425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 68 %\n"
          ]
        }
      ],
      "source": [
        "# testing the above trained model\n",
        "model.eval()\n",
        "\n",
        "# variables used to compute accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in subset_testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy on test set: %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HTxcxkCF5cS"
      },
      "source": [
        "####  PROBLEM 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sSYFx8WF5cS"
      },
      "source": [
        "Object Recognition with Vision Transformer: Implement and train an Encoder only Transformer\n",
        "(ViT-like) for the above object recognition task. In other words, implement multi-headed\n",
        "self-attention for the image classification (i.e., appending a < class > token to the image patches\n",
        "that are accepted as input tokens). Compare the performance of the two implementations (try to\n",
        "keep the number of parameters to be comparable and use the same amount of training and testing\n",
        "data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYz0lpN1F5cS"
      },
      "source": [
        "PREPARING DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8PdjB3AF5cT",
        "outputId": "3cc6bf6b-5ffa-4f8c-de1f-921365d84172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# applying transformation to data after downloading it\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((32, 32)),  # Resize images to match ViT input size\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=100, shuffle=True)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=100, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqHLlgB_F5cT"
      },
      "source": [
        "MULTIHEAD SELF-ATTENTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUXW27AsF5cT"
      },
      "source": [
        "In opposed to the self-attention class above where I tried to learn the representations of query, key and values using the convolution layer below here I tried to learn the representaions using linear layers for query, keys and values respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ye7HTw0gF5cU"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    # defining the parameters and layers\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # used linear layers for representations of query, key and values\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    # forward pass in multi head attention\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.shape[0]     # obtain the batch size\n",
        "        Query = self.query(query)\n",
        "        Key = self.key(key)\n",
        "        Value = self.value(value)\n",
        "\n",
        "        # reshape query, key and values to get multiple heads\n",
        "        Query = Query.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        Key = Key.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        Value = Value.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # computing attention scores\n",
        "        attnScores = torch.matmul(Query, Key.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            attnScores = attnScores.masked_fill(mask == 0, float('-1e20'))\n",
        "\n",
        "        smallOut = torch.softmax(attnScores, dim=-1)\n",
        "\n",
        "        # applied the softmax output to the values\n",
        "        output = torch.matmul(smallOut, Value)\n",
        "\n",
        "        # reshaping to concatenate heads\n",
        "        output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.head_dim * self.num_heads)\n",
        "\n",
        "        # applied linear lyaer befor final output\n",
        "        output = self.fc_out(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcp-g1bcF5cU"
      },
      "source": [
        "TRANSFORMER ENCODER LAYERS ARCHITECTURE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Xn9R0VF5cU"
      },
      "source": [
        "Because we need to use our own implemented MultiHead attention class in the layers of transformer so we need to create a class for defining transformer encoder layers also. This class is responsible for performing multi-head self attention, add and norm layer, adding residual connection, and for the linear layers in between. These layers are part of original ViT like transformer encoder blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "48TySxSNF5cV"
      },
      "outputs": [],
      "source": [
        "# transformer encoder architecture\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, num_layers, hidden_dim):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.ModuleList([\n",
        "                nn.LayerNorm(embed_dim),\n",
        "                nn.ReLU(),\n",
        "                MultiHeadAttention(embed_dim, num_heads),       # multihead attentoion class defined above\n",
        "                nn.LayerNorm(embed_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(embed_dim, hidden_dim),\n",
        "                nn.LayerNorm(hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, embed_dim)\n",
        "            ])\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    # forward pass in the encoder.\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            norm1, relu1, attention, norm2, relu2, fc1, norm3, relu3, fc2 = layer\n",
        "            residual = x\n",
        "            x = norm1(x)\n",
        "            x = relu1(x)\n",
        "            x = attention(x, x, x)\n",
        "            x = x + residual\n",
        "            residual = x\n",
        "            x = norm2(x)\n",
        "            x = relu2(x)\n",
        "            x = fc1(x)\n",
        "            x = norm3(x)\n",
        "            x = relu3(x)\n",
        "            x = fc2(x)\n",
        "            x = x + residual\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_55osRS7F5cV"
      },
      "source": [
        "TRANSFORMER CLASS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM-_FKy8F5cV"
      },
      "source": [
        "Uses the above defined transformer encoder class for the encoder block. The transformer block appends class token to image patches, computes positional encoding and adds it to the original input image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aaLIlEMgF5cW"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    # initalize necessary parameters.\n",
        "    def __init__(self, image_size, patch_size, num_classes, embed_dim, num_heads, num_layers, hidden_dim):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        # Computing the number of patches based of image size\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = 3 * patch_size ** 2         # 3 channels for RGB images\n",
        "\n",
        "        # embed_dim is a hyper-parameter which represents the dimensionality of token embedding\n",
        "        self.patch_embedding = nn.Linear(patch_dim, embed_dim)\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
        "        self.encoder = TransformerEncoder(embed_dim, num_heads, num_layers, hidden_dim)     # used the above defined class for Transformer encoder block\n",
        "        self.class_token = nn.Parameter(torch.randn(1, 1, embed_dim))   # append to learn global context of image\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    # forward pass in transformer\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.view(batch_size, -1, 3 * 4 * 4)\n",
        "        x = self.patch_embedding(x)\n",
        "\n",
        "        # appending or concatenating the one class tokens to each image of the batch\n",
        "        class_token = self.class_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat([class_token, x], dim=1)\n",
        "\n",
        "        # like in mormal transformer add positional encoding before passing input to transformer\n",
        "        x = x + self.positional_embedding\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        # get the class token and pass it thru the fc layer, because class token captures global context of image.and hence represent the entire image\n",
        "        x = x[:, 0]\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vAEQi3DF5cW"
      },
      "source": [
        "TRAINING THE MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZtnujVpF5cW"
      },
      "source": [
        "Initializing the model, loss function and the optimizer. Choice of optimizer, loss function, learning rate and various parameters remain same as in problem 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FfyDmWmF5cW",
        "outputId": "6f310fc9-254f-40be-8af9-f1845a895a7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   200] loss: 2.084\n",
            "[1,   400] loss: 1.726\n",
            "[2,   200] loss: 1.580\n",
            "[2,   400] loss: 1.539\n",
            "[3,   200] loss: 1.482\n",
            "[3,   400] loss: 1.469\n",
            "[4,   200] loss: 1.412\n",
            "[4,   400] loss: 1.412\n",
            "[5,   200] loss: 1.364\n",
            "[5,   400] loss: 1.363\n",
            "[6,   200] loss: 1.314\n",
            "[6,   400] loss: 1.328\n",
            "[7,   200] loss: 1.274\n",
            "[7,   400] loss: 1.295\n",
            "[8,   200] loss: 1.234\n",
            "[8,   400] loss: 1.271\n",
            "[9,   200] loss: 1.207\n",
            "[9,   400] loss: 1.226\n",
            "[10,   200] loss: 1.183\n",
            "[10,   400] loss: 1.185\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# training the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model2 = Transformer(image_size=32, patch_size=4, num_classes=10, embed_dim=64, num_heads=4, num_layers=3, hidden_dim=256)\n",
        "model2.to(device)\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer2.zero_grad()\n",
        "\n",
        "        outputs = model2(inputs)\n",
        "        loss = criterion2(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYdRYpmNF5cX"
      },
      "source": [
        "TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_XwhuDLF5cX",
        "outputId": "d2f1d165-c2b1-4473-d19c-9c03cf333c20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 52 %\n"
          ]
        }
      ],
      "source": [
        "# evaluating model accuracy\n",
        "model2.eval()\n",
        "correct_model2 = 0\n",
        "total_model2 = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model2(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_model2 += labels.size(0)\n",
        "        correct_model2 += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy on test set: %d %%' % (100 * correct_model2 / total_model2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP_dH-75F5cX"
      },
      "source": [
        "PERFORMANCE COMPARISON MODEL 1 (PROBLEM 1) V/S MODEL 2 (PROBLEM 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNloy9hnF5cY"
      },
      "source": [
        "* Cifar-10 dataset is a kind of dataset which is relatively simpler and in which the local features are sufficient, CNNWithSelfAttention is expected to perform better which is shown by the accuracy on test set. Accuracy on test set for CNN with self attention model is better than the transformer model. <br> <br>\n",
        "* Datasets which have more complex spatial relationships and long range dependencies, vision transformers can perform better then CNN on such datasets. <br> <br>\n",
        "* Overall, for the specified task of object recognintion on the cifar-10 dataset both the models are expected to perform well. However, CNN with self attention performs better may be due to the use of convolution layers while defining self attention class in CNN while using linear layer for attention in transformer model. But for capturing global dependencies, linear layers are more suitable so I used linear layers in attention for problem 2<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOTHcSOSF5cY"
      },
      "source": [
        "END OF ASSIGNMENT 3"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
